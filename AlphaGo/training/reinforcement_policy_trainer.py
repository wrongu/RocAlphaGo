import os, argparse, json
import numpy as np
from keras.optimizers import SGD
from AlphaGo.ai import ProbabilisticPolicyPlayer
import AlphaGo.go as go
from AlphaGo.go import GameState
from AlphaGo.models.policy import CNNPolicy
from AlphaGo.preprocessing.preprocessing import Preprocess
from AlphaGo.util import flatten_idx

def make_training_pairs(player, opp, features, mini_batch_size):
	"""Make training pairs for batch of matches, utilizing player.get_moves (parallel form of
	player.get_move), which calls `CNNPolicy.batch_eval_state`.

	Args:
	player -- player that we're always updating
	opp -- batch opponent
	feature_list -- game features to be one-hot encoded
	mini_batch_size -- number of games in mini-batch

	Return:
	X_list -- list of 1-hot board states associated with moves.
	y_list -- list of 1-hot moves associated with board states.
	winners -- list of winners associated with each game in batch
	"""

	def do_move(states, states_prev, moves, X_list, y_list, player_color):
		bsize_flat = bsize * bsize
		for st, st_prev, mv, X, y in zip(states, states_prev, moves, X_list,
										 y_list):
			if not st.is_end_of_game:
				# Only do more moves if not end of game already
				st.do_move(mv)
				if st.current_player != player_color and mv is not go.PASS_MOVE:
					# Convert move to one-hot
					state_1hot = preprocessor.state_to_tensor(st_prev)
					move_1hot = np.zeros(bsize_flat)
					move_1hot[flatten_idx(mv, bsize)] = 1
					X.append(state_1hot)
					y.append(move_1hot)
		return states, X_list, y_list

	# Lists of game training pairs (1-hot)
	X_list = [list() for _ in xrange(mini_batch_size)]
	y_list = [list() for _ in xrange(mini_batch_size)]
	preprocessor = Preprocess(features)
	bsize = player.policy.model.input_shape[-1]
	states = [GameState() for i in xrange(mini_batch_size)]
	# Randomly choose who goes first (i.e. color of 'player')
	player_color = np.random.choice([go.BLACK, go.WHITE])
	player1, player2 = (player, opp) if player_color == go.BLACK else \
		(opp, player)
	while True:
		# Cache states before moves
		states_prev = [st.copy() for st in states]
		# Get moves (batch)
		moves_black = player1.get_moves(states)
		# Do moves (black)
		states, X_list, y_list = do_move(states, states_prev, moves_black,
										 X_list, y_list, player_color)
		# Do moves (white)
		moves_white = player2.get_moves(states)
		states, X_list, y_list = do_move(states, states_prev, moves_white,
										 X_list, y_list, player_color)
		# If all games have ended, we're done. Get winners.
		done = [st.is_end_of_game for st in states]
		if all(done):
			break
	winners = [st.get_winner() for st in states]
	# Concatenate tensors across turns within each game
	for i in xrange(mini_batch_size):
		X_list[i] = np.concatenate(X_list[i], axis=0)
		y_list[i] = np.vstack(y_list[i])
	return X_list, y_list, winners

def train_batch(player, X_list, y_list, winners, lr):
	"""Given the outcomes of a mini-batch of play against a fixed opponent,
	   update the weights with reinforcement learning in place.

	   Args:
	   player -- player object with policy weights to be updated
	   X_list -- List of one-hot encoded states.
	   y_list -- List of one-hot encoded actions (to pair with X_list).
	   winners -- List of winners corresponding to each item in
				  training_pairs_list
	   lr -- Keras learning rate
	"""

	for X, y, winner in zip(X_list, y_list, winners):
		# Update weights in + direction if player won, and - direction if player lost.
		# Setting learning rate negative is hack for negative weights update.
		if winner == -1:
			player.policy.model.optimizer.lr.set_value(-lr)
		else:
			player.policy.model.optimizer.lr.set_value(lr)
		player.policy.model.fit(X, y, nb_epoch=1, batch_size=len(X))

def run(player, args, opponents, features, model_folder):
	# Set SGD and compile
	sgd = SGD(lr=args.learning_rate)
	player.policy.model.compile(loss='binary_crossentropy', optimizer=sgd)
	player_wins_per_batch = []
	for i_iter in xrange(args.iterations):
		# Train mini-batches
		for i_batch in xrange(args.save_every):
			# Randomly choose opponent from pool
			opp_filepath = np.random.choice(os.listdir(model_folder))
			opp_path = os.path.join(model_folder,opp_filepath)
			opp = CNNPolicy.create_network().load_weights(opp_path)
			# Make training pairs and do RL
			X_list, y_list, winners = make_training_pairs(
				player, opp, features, args.game_batch_size)
			n_wins = np.sum(np.array(winners) == 1)
			player_wins_per_batch.append(n_wins)
			print 'Number of wins this batch: {}/{}'.format(n_wins, args.game_batch_size)
			train_batch(player, X_list, y_list, winners, args.learning_rate)
		# Save intermediate models
		model_path = os.path.join(model_folder,str(i_iter))
		player.policy.model.save_weights(model_path)

if __name__ == '__main__':
	parser = argparse.ArgumentParser(description='Perform reinforcement learning '
									 'to improve given policy network. Second phase of pipeline.')
	parser.add_argument("initial_weights", help="Path to file with weights to start from.")
	parser.add_argument("initial_json", help="Path to file with initial network params.")
	parser.add_argument("model_folder", help="Path to folder where the model"
						" params will be saved after each epoch.")
	parser.add_argument(
		"--learning_rate", help="Keras learning rate (Default: .03)",
		          type=float, default=.03)
	parser.add_argument(
		"--save_every", help="Save policy every n mini-batches (Default: 500)",
		          type=int, default=500)
	parser.add_argument(
		"--game_batch_size", help="Number of games per mini-batch (Default: 20)",
		          type=int, default=20)
	parser.add_argument(
		"--iterations", help="Number of training iterations (i.e. mini-batch) "
		"(Default: 20)", type=int, default=20)
	args = parser.parse_args()

	# Set initial conditions
	policy = CNNPolicy.load_model(args.initial_json)
	policy.model.load_weights(args.initial_weights)
	player = ProbabilisticPolicyPlayer(policy.model)

	opponent_files = os.listdir(args.model_folder)
	if len(opponent_files) == 0: # Start new RL training session
		opponents = [player]
	else:						 # Resume existing RL training session
		opponents = opponent_files

	with open(args.initial_json) as j:
		features = json.load(j).feature_list

	run(player, args, opponents, features, args.model_folder)
